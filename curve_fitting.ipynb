{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curve fitting\n",
    "Through curve fitting exercise, you will understand the basics of regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "import default_settings as default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load samples.\n",
    "with open('data/regression_samples.txt') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "X = [float(line.split()[0]) for line in lines if len(line.split()) > 0]\n",
    "Y = [float(line.split()[1]) for line in lines if len(line.split()) > 0]\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "# display the points.\n",
    "plt.scatter(X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## problem description\n",
    "You will find a curve that fit the above scatter the best.   \n",
    "Let's assume the curve is described as: f(x, w) = a + b * x + c * x^2 + d * x^3.  \n",
    "Your task is to find w = [a, b, c, d].   \n",
    "  \n",
    "Here is our approach.  \n",
    "\n",
    "1. Initialize w with some numbers.\n",
    "2. Calculate how good f(x, w) fits the scatters by defining the cost function. For now, cost function is the root mean square error devided by the number of points. \n",
    "3. Update w using gradient descent, until the cost becomes small enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilize w.\n",
    "w = [1, 1, 1, 1]\n",
    "\n",
    "# define function. \n",
    "def f_(x, w):\n",
    "    ### WRITE YOUR OWN CODES ### \n",
    "    #y = w[0] + w[1] * x + w[2] * np.power(x,2) + w[3] * np.power(x,3)\n",
    "    y = w[0] + w[1] * x + w[2] * x ** 2 + w[3] * x ** 3\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "# calculate predictions y\n",
    "Y_predict = f_(X, w)\n",
    "# display the points.\n",
    "plt.plot(X,Y_predict,'r-')\n",
    "plt.plot(X,Y,'b.')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost function.\n",
    "def cost_function(w, X, Y):\n",
    "    ### WRITE YOUR OWN CODES ### \n",
    "    m = len(Y)\n",
    "    \n",
    "    Y_predict = f_(X, w)\n",
    "    cost = (1/(2*m)) * np.sum(np.square(Y_predict-Y))\n",
    "    \n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = cost_function(w, X, Y)\n",
    "print(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update w using gradient descent. \n",
    "def update_w(w, X, Y, learning_rate=0.01, iterations=100):\n",
    "    ### WRITE YOUR OWN CODES ### \n",
    "    m = len(Y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    w_history = np.zeros((iterations,4))\n",
    "    \n",
    "    for it in range(iterations):\n",
    "        \n",
    "        Y_predict = f_(X, w)\n",
    "        \n",
    "        # if f = a + bx, the below equation is correct.\n",
    "        # but in our case, X * (Y_pred - Y) does not work. Why?\n",
    "        w = w -(1/m)*learning_rate*( X.T.dot((Y_predict - Y)))\n",
    "        \n",
    "        w_history[it,:] =w.T\n",
    "        cost_history[it]  = cost_function(w,X,Y)\n",
    "    \n",
    "    return w, cost_history, w_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w)\n",
    "learning_rate = 0.005\n",
    "iteration = 1000\n",
    "\n",
    "w0, cost_history, w_history = update_w(w, X, Y, learning_rate, iteration)\n",
    "print(w0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "ax.set_ylabel('J(w)')\n",
    "ax.set_xlabel('Iterations')\n",
    "_=ax.plot(range(iteration),cost_history,'b-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate predictions y\n",
    "Y_predict2 = f_(X, w0)\n",
    "# display the points.\n",
    "plt.plot(X,Y_predict2,'r-')\n",
    "plt.plot(X,Y,'b.')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test existing function\n",
    "coe = np.polyfit(X, Y, 3)\n",
    "print(coe)\n",
    "Y_fit = coe[0] * X ** 3 + coe[1] * X ** 2 + coe[2] * X + coe[3]\n",
    "plt.plot(X,Y_fit,'r-')\n",
    "plt.plot(X,Y,'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hint: Gredient descent.\n",
    "To make the situation easier, let's update w only once.   \n",
    "Let's denote cost_function as J(w, X, Y) = 1/(2m) * (Y_pred - Y)^2.  \n",
    "In Gredient descient, update w as w- learning_rate * dJ/dw (partial derivative).  \n",
    "dJ/dw = (Y_pred - Y)/m * d(Y_pred - Y)/dw  \n",
    "\n",
    "in our case y = w[0] + w[1] * x + w[2] * x ** 2 + w[3] * x ** 3, therefore:  \n",
    "- dJ[0] = (Y_predict - Y)/m\n",
    "- dJ[1] = (Y_predict - Y)/m * x\n",
    "- dJ[2] = (Y_predict - Y)/m * x**2\n",
    "- dJ[3] = (Y_predict - Y)/m * x**3\n",
    "\n",
    "Reference: \n",
    "- https://mathwords.net/saikyukouka\n",
    "- https://qiita.com/Takayoshi_Makabe/items/ee467313c38b1879c097"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_w1(w, X, Y, learning_rate = 0.001):\n",
    "    m = len(Y)\n",
    "    \n",
    "    # w to update.\n",
    "    w_ = np.copy(np.array(w))\n",
    "    \n",
    "    ### WRITE YOUR OWN CODES: BEGIN ### \n",
    "    Y_predict = f_(X, w)\n",
    "    error = Y_predict - Y\n",
    "    w_[0] = None # change here\n",
    "    w_[1] = w[1] - learning_rate /m * np.dot(X.T, error)\n",
    "    w_[2] = None # change here\n",
    "    w_[3] = None # change here\n",
    "    ### WRITE YOUR OWN CODES: END ### \n",
    "    \n",
    "    return w_\n",
    "\n",
    "# gradient descent loop.\n",
    "def gradient_descent(w0, X, Y, learning_rate = 0.001, n_iter = 100):    \n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    w = np.copy(w0)\n",
    "    for i in range(n_iter):\n",
    "        ### WRITE YOUR OWN CODES: BEGIN ### \n",
    "        # update w.\n",
    "        None\n",
    "        \n",
    "        # add value to J_history and w_history.\n",
    "        None \n",
    "        \n",
    "        # update w.\n",
    "        None\n",
    "        \n",
    "        ### WRITE YOUR OWN CODES: END ###\n",
    "        \n",
    "    return w_history, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check\n",
    "# display cost function.\n",
    "w = np.ones(4)\n",
    "w_history, J_history = gradient_descent(w, X, Y, learning_rate = 0.0001, n_iter = 1000)\n",
    "plt.plot(J_history)\n",
    "print('at iteration {}, J takes mininum value.'.format(np.argmin(J_history)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## display curve.\n",
    "# get the min value of J.\n",
    "w_best = w_history[np.argmin(J_history)]\n",
    "Y_pred = f_(X, w_best)\n",
    "\n",
    "plt.plot(X, Y_pred, 'r-')\n",
    "plt.plot(X, Y, 'b.')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
